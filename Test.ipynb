{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c33da27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email sent successfully !\n"
     ]
    }
   ],
   "source": [
    "import smtplib\n",
    "from email.message import EmailMessage\n",
    "import config\n",
    "import imaplib, email\n",
    "from time import sleep\n",
    "\n",
    "def Send_email(mail = \"youssefkhalifa@51talk.com\"):\n",
    "\n",
    "    msg = EmailMessage()\n",
    "    msg[\"Subject\"] = \"Test from Python\"\n",
    "    msg[\"From\"] = config.EMAIL\n",
    "    msg[\"To\"] = mail\n",
    "    msg.set_content(\"Submission is done successfully !\")\n",
    "\n",
    "    with smtplib.SMTP_SSL(config.SMTP_HOST, config.SMTP_PORT) as smtp:\n",
    "        smtp.login(config.EMAIL, config.PASSWORD)\n",
    "        smtp.send_message(msg)\n",
    "    print('Email sent successfully !')\n",
    "\n",
    "def Check_for_mail():\n",
    "    \n",
    "    with imaplib.IMAP4_SSL(config.IMAP_HOST, config.IMAP_PORT) as imap:\n",
    "        imap.login(config.EMAIL, config.PASSWORD)\n",
    "        imap.select(\"INBOX\")  # or another folder\n",
    "        status, data = imap.search(None, 'UNSEEN')  # e.g., unseen messages\n",
    "        for num in data[0].split():\n",
    "            status, msg_data = imap.fetch(num, \"(RFC822)\")\n",
    "            msg = email.message_from_bytes(msg_data[0][1])\n",
    "            print(\"From:\", msg.get(\"From\"))\n",
    "            print(\"Subject:\", msg.get(\"Subject\"))\n",
    "            # If the email is multipart, pull out the first text/plain part\n",
    "            if msg.is_multipart():\n",
    "                for part in msg.walk():\n",
    "                    if part.get_content_type() == \"text/plain\":\n",
    "                        print(part.get_payload(decode=True).decode(part.get_content_charset() or \"utf-8\"))\n",
    "                        break\n",
    "            else:\n",
    "                print(msg.get_payload(decode=True).decode(msg.get_content_charset() or \"utf-8\"))\n",
    "\n",
    "Send_email(mail=\"youssefkhalifa458@gmail.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2199393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df =pd.read_excel('Assets/Team Mapping & Contacts.xlsx')\n",
    "df.to_csv('Assets/Team Mapping & Contacts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d150ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Column, Integer, String, Numeric, text\n",
    "from sqlalchemy.orm import DeclarativeBase, sessionmaker, Mapped, mapped_column\n",
    "from datetime import datetime\n",
    "\n",
    "# engine (uses psycopg)\n",
    "engine = create_engine(\"postgresql+psycopg://postgres:postgres@localhost:5432/appdb\", echo=False)\n",
    "\n",
    "class Base(DeclarativeBase):\n",
    "    pass\n",
    "\n",
    "class Widget(Base):\n",
    "    __tablename__ = \"widgets\"\n",
    "    id: Mapped[int] = mapped_column(Integer, primary_key=True)\n",
    "    name: Mapped[str] = mapped_column(String, nullable=False)\n",
    "    price: Mapped[float] = mapped_column(Numeric(10,2), nullable=False)\n",
    "\n",
    "# create tables\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "Session = sessionmaker(bind=engine)\n",
    "with Session() as s:\n",
    "    # create\n",
    "    w = Widget(name=\"doodad\", price=14.50)\n",
    "    s.add(w)\n",
    "    s.commit()\n",
    "    s.refresh(w)\n",
    "    print(\"Inserted:\", w.id)\n",
    "\n",
    "    # read\n",
    "    recent = s.query(Widget).order_by(Widget.id.desc()).limit(5).all()\n",
    "    for r in recent:\n",
    "        print(r.id, r.name, r.price)\n",
    "\n",
    "    # raw SQL when you want it\n",
    "    for (now,) in s.execute(text(\"SELECT now()\")):\n",
    "        print(now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6e7fb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database ready ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# create_db_then_run.py\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import ProgrammingError\n",
    "\n",
    "PG_USER = \"postgres\"              # your superuser (or a role with CREATEDB)\n",
    "PG_PASS = \"123\"         # <-- put the real password here\n",
    "PG_HOST = \"localhost\"\n",
    "PG_PORT = 5432\n",
    "APP_DB  = \"appdb\"\n",
    "\n",
    "# 1) Connect to the built-in 'postgres' database as admin\n",
    "admin_url = f\"postgresql+psycopg://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/postgres\"\n",
    "admin_engine = create_engine(admin_url, isolation_level=\"AUTOCOMMIT\")\n",
    "\n",
    "with admin_engine.connect() as conn:\n",
    "    exists = conn.execute(\n",
    "        text(\"SELECT 1 FROM pg_database WHERE datname = :n\"),\n",
    "        {\"n\": APP_DB},\n",
    "    ).scalar()\n",
    "    if not exists:\n",
    "        conn.execute(text(f\"CREATE DATABASE {APP_DB}\"))\n",
    "        # Optionally assign an owner:\n",
    "        # conn.execute(text(f\"ALTER DATABASE {APP_DB} OWNER TO {PG_USER}\"))\n",
    "\n",
    "# 2) Now connect to your actual app database\n",
    "app_url = f\"postgresql+psycopg://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{APP_DB}\"\n",
    "app_engine = create_engine(app_url)\n",
    "\n",
    "print(\"Database ready ‚úÖ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cc9fa41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission: 2 Alice Ahmed\n",
      "in_probation: True\n",
      "notice_period_days: 45\n",
      "created_at: 2025-10-29 13:34:17.703922+03:00 updated_at: 2025-10-29 13:34:17.703922+03:00\n",
      "updated_at after update: 2025-10-29 13:34:17.721702+03:00\n"
     ]
    }
   ],
   "source": [
    "# file: hr_copilot_models.py\n",
    "from datetime import date, datetime  # add datetime\n",
    "from typing import Optional\n",
    "from sqlalchemy import (\n",
    "    create_engine, text, MetaData, Table, Index, DDL, event,\n",
    "    Integer, String, Boolean, Date, TIMESTAMP, func, ForeignKey, Computed, Text, UniqueConstraint\n",
    ")\n",
    "from sqlalchemy.dialects import postgresql as psql\n",
    "from sqlalchemy.orm import DeclarativeBase, mapped_column, Mapped, relationship, Session\n",
    "from sqlalchemy.schema import ForeignKeyConstraint\n",
    "\n",
    "\n",
    "# ---- 0) Engine ----\n",
    "# Adjust to your settings\n",
    "DATABASE_URL = \"postgresql+psycopg://postgres:123@localhost:5432/appdb\"\n",
    "engine = create_engine(DATABASE_URL, echo=False, future=True)\n",
    "\n",
    "# ---- 1) Bootstrap objects that must exist before tables ----\n",
    "# Create role_t enum type if missing (safe via DO/EXCEPTION)\n",
    "BOOTSTRAP_ENUM = \"\"\"\n",
    "DO $$\n",
    "BEGIN\n",
    "  CREATE TYPE role_t AS ENUM ('super_user','hr','leader','chm','it','admin');\n",
    "EXCEPTION WHEN duplicate_object THEN NULL;\n",
    "END $$;\n",
    "\"\"\"\n",
    "\n",
    "# Create/replace the set_updated_at() trigger function\n",
    "BOOTSTRAP_FUNC = \"\"\"\n",
    "CREATE OR REPLACE FUNCTION set_updated_at() RETURNS trigger AS $$\n",
    "BEGIN\n",
    "  NEW.updated_at := now();\n",
    "  RETURN NEW;\n",
    "END;\n",
    "$$ LANGUAGE plpgsql;\n",
    "\"\"\"\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    conn.exec_driver_sql(BOOTSTRAP_ENUM)\n",
    "    conn.exec_driver_sql(BOOTSTRAP_FUNC)\n",
    "\n",
    "# We‚Äôll reference the already-created PostgreSQL enum\n",
    "role_t = psql.ENUM(\n",
    "    \"super_user\",\"hr\",\"leader\",\"chm\",\"it\",\"admin\",\n",
    "    name=\"role_t\", create_type=False\n",
    ")\n",
    "\n",
    "# ---- 2) ORM models / metadata ----\n",
    "class Base(DeclarativeBase):\n",
    "    metadata = MetaData()\n",
    "\n",
    "class User(Base):\n",
    "    __tablename__ = \"users\"\n",
    "\n",
    "    id: Mapped[int] = mapped_column(Integer, primary_key=True)\n",
    "    email: Mapped[str] = mapped_column(String(150), unique=True, nullable=False)\n",
    "    full_name: Mapped[str] = mapped_column(String(120), nullable=False)\n",
    "    role: Mapped[str] = mapped_column(role_t, nullable=False)\n",
    "    password_hash: Mapped[Optional[str]] = mapped_column(String, nullable=True)\n",
    "    is_active: Mapped[bool] = mapped_column(Boolean, nullable=False, server_default=text(\"TRUE\"))\n",
    "    created_at: Mapped[datetime] = mapped_column(TIMESTAMP(timezone=True), nullable=False, server_default=func.now())\n",
    "    hashed_password = mapped_column(String, nullable=False)\n",
    "\n",
    "\n",
    "class Submission(Base):\n",
    "    __tablename__ = \"submissions\"\n",
    "\n",
    "    id: Mapped[int] = mapped_column(Integer, primary_key=True)\n",
    "\n",
    "    employee_name: Mapped[str] = mapped_column(String(100), nullable=False)\n",
    "    employee_email: Mapped[str] = mapped_column(String(150), nullable=False)\n",
    "\n",
    "    joining_date: Mapped[date] = mapped_column(Date, nullable=False)\n",
    "    submission_date: Mapped[date] = mapped_column(Date, nullable=False, server_default=text(\"CURRENT_DATE\"))\n",
    "    last_working_day: Mapped[date] = mapped_column(Date, nullable=False)\n",
    "\n",
    "    resignation_status: Mapped[str] = mapped_column(String(30), nullable=False, server_default=text(\"'submitted'\"))\n",
    "    team_leader_reply: Mapped[Optional[bool]] = mapped_column(Boolean)\n",
    "    team_leader_notes: Mapped[Optional[str]] = mapped_column(Text)  # Text is fine for notes\n",
    "    chinese_head_reply: Mapped[Optional[bool]] = mapped_column(Boolean)\n",
    "    chinese_head_notes: Mapped[Optional[str]] = mapped_column(Text)\n",
    "\n",
    "    exit_interview_status: Mapped[str] = mapped_column(String(30), nullable=False, server_default=text(\"'not_scheduled'\"))\n",
    "    exit_interview_notes: Mapped[Optional[str]] = mapped_column(Text)\n",
    "\n",
    "    it_support_reply: Mapped[Optional[bool]] = mapped_column(Boolean)\n",
    "\n",
    "    vendor_mail_sent: Mapped[bool] = mapped_column(Boolean, nullable=False, server_default=text(\"FALSE\"))\n",
    "    medical_card_collected: Mapped[bool] = mapped_column(Boolean, nullable=False, server_default=text(\"FALSE\"))\n",
    "\n",
    "    in_probation: Mapped[bool] = mapped_column(\n",
    "        Boolean, Computed(\"(submission_date - joining_date) < 90\", persisted=True)\n",
    "    )\n",
    "    notice_period_days: Mapped[int] = mapped_column(\n",
    "        Integer, Computed(\"(last_working_day - submission_date)\", persisted=True)\n",
    "    )\n",
    "\n",
    "    created_at: Mapped[datetime] = mapped_column(TIMESTAMP(timezone=True), nullable=False, server_default=func.now())\n",
    "    updated_at: Mapped[datetime] = mapped_column(TIMESTAMP(timezone=True), nullable=False, server_default=func.now())\n",
    "\n",
    "    assets: Mapped[\"Assets\"] = relationship(\n",
    "        back_populates=\"submission\", uselist=False, cascade=\"all, delete-orphan\", passive_deletes=True\n",
    "    )\n",
    "\n",
    "    __table_args__ = (\n",
    "        Index(\"idx_submissions_status\", \"resignation_status\"),\n",
    "        Index(\"idx_submissions_employee_eml\", \"employee_email\"),\n",
    "    )\n",
    "\n",
    "\n",
    "class Assets(Base):\n",
    "    __tablename__ = \"assets\"\n",
    "\n",
    "    id: Mapped[int] = mapped_column(Integer, primary_key=True)\n",
    "    res_id: Mapped[int] = mapped_column(Integer, nullable=False)\n",
    "\n",
    "    laptop: Mapped[Optional[bool]] = mapped_column(Boolean)\n",
    "    mouse: Mapped[Optional[bool]] = mapped_column(Boolean)\n",
    "    headphones: Mapped[Optional[bool]] = mapped_column(Boolean)\n",
    "    others: Mapped[Optional[str]] = mapped_column(Text)\n",
    "    approved: Mapped[Optional[bool]] = mapped_column(Boolean)\n",
    "\n",
    "    created_at: Mapped[datetime] = mapped_column(TIMESTAMP(timezone=True), nullable=False, server_default=func.now())\n",
    "    updated_at: Mapped[datetime] = mapped_column(TIMESTAMP(timezone=True), nullable=False, server_default=func.now())\n",
    "\n",
    "    __table_args__ = (\n",
    "        ForeignKeyConstraint([\"res_id\"], [\"submissions.id\"], deferrable=True, initially=\"DEFERRED\"),\n",
    "        UniqueConstraint(\"res_id\", name=\"uq_assets_res_id\"),  # enforces one-to-one at DB level\n",
    "        Index(\"idx_assets_res_id\", \"res_id\"),\n",
    "    )\n",
    "\n",
    "    submission: Mapped[\"Submission\"] = relationship(back_populates=\"assets\", single_parent=True)\n",
    "\n",
    "\n",
    "# ---- 3) Create tables ----\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "# ---- 4) Triggers: keep updated_at fresh on UPDATE (matches your DDL) ----\n",
    "# Drop+Create triggers for submissions\n",
    "submissions_tbl: Table = Submission.__table__\n",
    "assets_tbl: Table = Assets.__table__\n",
    "\n",
    "trg_drop_create_submissions = DDL(\"\"\"\n",
    "DROP TRIGGER IF EXISTS trg_submissions_updated ON submissions;\n",
    "CREATE TRIGGER trg_submissions_updated\n",
    "BEFORE UPDATE ON submissions\n",
    "FOR EACH ROW EXECUTE FUNCTION set_updated_at();\n",
    "\"\"\")\n",
    "\n",
    "trg_drop_create_assets = DDL(\"\"\"\n",
    "DROP TRIGGER IF EXISTS trg_assets_updated ON assets;\n",
    "CREATE TRIGGER trg_assets_updated\n",
    "BEFORE UPDATE ON assets\n",
    "FOR EACH ROW EXECUTE FUNCTION set_updated_at();\n",
    "\"\"\")\n",
    "\n",
    "# Ensure triggers exist after (re)creating tables\n",
    "event.listen(submissions_tbl, \"after_create\", trg_drop_create_submissions)\n",
    "event.listen(assets_tbl, \"after_create\", trg_drop_create_assets)\n",
    "\n",
    "# (Re-)run the trigger DDL now that tables exist\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"DROP TRIGGER IF EXISTS trg_submissions_updated ON submissions\"))\n",
    "    conn.execute(text(\"CREATE TRIGGER trg_submissions_updated BEFORE UPDATE ON submissions FOR EACH ROW EXECUTE FUNCTION set_updated_at()\"))\n",
    "    conn.execute(text(\"DROP TRIGGER IF EXISTS trg_assets_updated ON assets\"))\n",
    "    conn.execute(text(\"CREATE TRIGGER trg_assets_updated BEFORE UPDATE ON assets FOR EACH ROW EXECUTE FUNCTION set_updated_at()\"))\n",
    "\n",
    "# ---- 5) Tiny demo: insert / query / see generated columns ----\n",
    "if __name__ == \"__main__\":\n",
    "    with Session(engine) as s:\n",
    "        # create a submission\n",
    "        sub = Submission(\n",
    "            employee_name=\"Alice Ahmed\",\n",
    "            employee_email=\"alice@example.com\",\n",
    "            joining_date=date(2025, 8, 1),\n",
    "            submission_date=date(2025, 10, 1),\n",
    "            last_working_day=date(2025, 11, 15),\n",
    "            resignation_status=\"submitted\"\n",
    "        )\n",
    "        s.add(sub)\n",
    "        s.flush()  # get sub.id\n",
    "\n",
    "        # add assets (one-to-one)\n",
    "        assets = Assets(res_id=sub.id, laptop=True, mouse=True, headphones=False, approved=None, others=\"Returned badge\")\n",
    "        s.add(assets)\n",
    "        s.commit()\n",
    "\n",
    "        # query + show generated columns and timestamps\n",
    "        fresh = s.get(Submission, sub.id)\n",
    "        print(\"Submission:\", fresh.id, fresh.employee_name)\n",
    "        print(\"in_probation:\", fresh.in_probation)          # boolean from generated column\n",
    "        print(\"notice_period_days:\", fresh.notice_period_days)\n",
    "        print(\"created_at:\", fresh.created_at, \"updated_at:\", fresh.updated_at)\n",
    "\n",
    "        # trigger demo: update notes -> updated_at changes\n",
    "        fresh.team_leader_notes = \"Looks good.\"\n",
    "        s.commit()\n",
    "        s.refresh(fresh)\n",
    "        print(\"updated_at after update:\", fresh.updated_at)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae0cfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema created/updated successfully.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "DATABASE_URL = \"postgresql+psycopg://postgres:123@localhost:5432/appdb\"\n",
    "engine = create_engine(DATABASE_URL, echo=False, future=True)\n",
    "\n",
    "stmts = [\n",
    "    # --- tables ---\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS exit_interviews (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        submission_id INTEGER UNIQUE NOT NULL\n",
    "            REFERENCES submissions(id) ON DELETE CASCADE,\n",
    "\n",
    "        -- Scheduling Information\n",
    "        scheduled_date TIMESTAMP,\n",
    "        scheduled_time VARCHAR(10),  -- HH:MM format\n",
    "        location VARCHAR(200),\n",
    "        interviewer VARCHAR(100),\n",
    "\n",
    "        -- Interview Details\n",
    "        interview_completed BOOLEAN DEFAULT FALSE,\n",
    "        interview_feedback TEXT,\n",
    "        interview_rating INTEGER,  -- 1-5 scale\n",
    "        interview_type VARCHAR(50),  -- in-person, virtual, phone\n",
    "\n",
    "        -- Follow-up Actions\n",
    "        hr_notes TEXT,\n",
    "        it_notification_sent BOOLEAN DEFAULT FALSE,\n",
    "\n",
    "        -- Timestamps\n",
    "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "        interview_completed_at TIMESTAMP\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS exit_interview_reminders (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        exit_interview_id INTEGER NOT NULL\n",
    "            REFERENCES exit_interviews(id) ON DELETE CASCADE,\n",
    "\n",
    "        -- Reminder Type\n",
    "        reminder_type VARCHAR(50) NOT NULL,  -- schedule_interview, submit_feedback, employee_reminder\n",
    "\n",
    "        -- Status\n",
    "        sent BOOLEAN DEFAULT FALSE,\n",
    "        sent_at TIMESTAMP,\n",
    "        scheduled_for TIMESTAMP NOT NULL,\n",
    "\n",
    "        -- Email details\n",
    "        recipient_email VARCHAR(150) NOT NULL,\n",
    "        recipient_name VARCHAR(100) NOT NULL,\n",
    "\n",
    "        -- Response tracking\n",
    "        responded BOOLEAN DEFAULT FALSE,\n",
    "        responded_at TIMESTAMP,\n",
    "\n",
    "        -- Timestamps\n",
    "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "    );\n",
    "    \"\"\",\n",
    "\n",
    "    # --- indexes ---\n",
    "    \"CREATE INDEX IF NOT EXISTS idx_exit_interviews_submission_id ON exit_interviews(submission_id);\",\n",
    "    \"CREATE INDEX IF NOT EXISTS idx_exit_interviews_scheduled_date ON exit_interviews(scheduled_date);\",\n",
    "    \"CREATE INDEX IF NOT EXISTS idx_exit_interview_reminders_interview_id ON exit_interview_reminders(exit_interview_id);\",\n",
    "    \"CREATE INDEX IF NOT EXISTS idx_exit_interview_reminders_scheduled_for ON exit_interview_reminders(scheduled_for);\",\n",
    "\n",
    "    # --- trigger function (separate statement because it includes $$...$$ with semicolons inside) ---\n",
    "    \"\"\"\n",
    "    CREATE OR REPLACE FUNCTION update_updated_at_column()\n",
    "    RETURNS TRIGGER AS $$\n",
    "    BEGIN\n",
    "        NEW.updated_at = CURRENT_TIMESTAMP;\n",
    "        RETURN NEW;\n",
    "    END;\n",
    "    $$ LANGUAGE plpgsql;\n",
    "    \"\"\",\n",
    "\n",
    "    # --- triggers (drop-if-exists for idempotency, then create) ---\n",
    "    \"DROP TRIGGER IF EXISTS update_exit_interviews_updated_at ON exit_interviews;\",\n",
    "    \"\"\"\n",
    "    CREATE TRIGGER update_exit_interviews_updated_at\n",
    "        BEFORE UPDATE ON exit_interviews\n",
    "        FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();\n",
    "    \"\"\",\n",
    "    \"DROP TRIGGER IF EXISTS update_exit_interview_reminders_updated_at ON exit_interview_reminders;\",\n",
    "    \"\"\"\n",
    "    CREATE TRIGGER update_exit_interview_reminders_updated_at\n",
    "        BEFORE UPDATE ON exit_interview_reminders\n",
    "        FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Run everything in one transaction\n",
    "with engine.begin() as conn:\n",
    "    for sql in stmts:\n",
    "        conn.exec_driver_sql(sql)\n",
    "\n",
    "print(\"Schema created/updated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "097512fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "employee_name\n",
      "employee_email\n",
      "joining_date\n",
      "submission_date\n",
      "last_working_day\n",
      "resignation_status\n",
      "team_leader_reply\n",
      "team_leader_notes\n",
      "chinese_head_reply\n",
      "chinese_head_notes\n",
      "exit_interview_status\n",
      "exit_interview_notes\n",
      "it_support_reply\n",
      "vendor_mail_sent\n",
      "medical_card_collected\n",
      "in_probation\n",
      "notice_period_days\n",
      "created_at\n",
      "updated_at\n",
      "last_reminded_at\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import inspect\n",
    "\n",
    "inspector = inspect(engine)\n",
    "for column in inspector.get_columns(\"submissions\", schema=\"public\"):\n",
    "    print(column[\"name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de5c55be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200\n",
      "Response: {'employee_name': 'Youssef Khalifa', 'employee_email': 'kalam11@company.com', 'joining_date': '2024-01-01T00:00:00', 'submission_date': '2024-11-02T00:00:00', 'last_working_day': '2024-12-31T00:00:00', 'id': 65, 'resignation_status': 'submitted', 'exit_interview_status': 'not_scheduled', 'team_leader_reply': None, 'chinese_head_reply': None, 'it_support_reply': None, 'medical_card_collected': False, 'vendor_mail_sent': False, 'team_leader_notes': None, 'chinese_head_notes': None, 'exit_interview_notes': None, 'in_probation': False, 'notice_period_days': 59, 'created_at': '2025-11-02T11:47:25.312406Z', 'updated_at': '2025-11-02T11:47:25.312406Z'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(\"http://localhost:8000/api/submission\",\n",
    "                          json={\n",
    "      \"employee_name\": \"Youssef Khalifa\",\n",
    "      \"employee_email\": \"kalam11@company.com\",\n",
    "      \"joining_date\": \"2024-01-01T00:00:00\",\n",
    "      \"submission_date\": \"2024-11-02T00:00:00\",\n",
    "      \"last_working_day\": \"2024-12-31T00:00:00\",\n",
    "      \"department\": \"AIPB\",\n",
    "      \"position\": \"AIBP\",\n",
    "      \"leader_name\": \"Youssef Khalifa\",\n",
    "      \"reason\": \"Test with leader name only\"\n",
    "  })\n",
    "\n",
    "print(f\"Status: {response.status_code}\")\n",
    "print(f\"Response: {response.json()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b436f42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è  PREPARING TO EMPTY SUBMISSIONS TABLE\n",
      "==================================================\n",
      "[DB] Database session closed in 0.000s\n",
      "[DB] Database session obtained in 0.000s\n",
      "[DB] Connection checked out from pool\n",
      "üìä Current Records Count:\n",
      "  Submissions: 1\n",
      "  Assets: 0\n",
      "  Exit Interviews: 0\n",
      "  Interview Reminders: 0\n",
      "\n",
      "‚ö†Ô∏è  WARNING: About to delete ALL submission records!\n",
      "‚úÖ Deleted interview reminders\n",
      "‚úÖ Deleted exit interviews\n",
      "‚úÖ Deleted assets\n",
      "‚úÖ Deleted submissions\n",
      "[DB] Connection returned to pool\n",
      "\n",
      "üéâ SUCCESS! All submission records have been deleted.\n",
      "[DB] Connection checked out from pool\n",
      "üìä Remaining submissions: 0\n",
      "[DB] Connection returned to pool\n",
      "üîê Database connection closed\n",
      "==================================================\n",
      "‚úÖ Operation completed!\n"
     ]
    }
   ],
   "source": [
    "# Empty the submissions table entirely\n",
    "# WARNING: This will delete ALL records permanently!\n",
    "\n",
    "import pandas as pd\n",
    "from app.database import engine, get_db\n",
    "from app.models.submission import Submission\n",
    "from app.models.asset import Asset\n",
    "from app.models.exit_interview import ExitInterview, ExitInterviewReminder\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import text\n",
    "\n",
    "print(\"üóëÔ∏è  PREPARING TO EMPTY SUBMISSIONS TABLE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get database session\n",
    "db_gen = get_db()\n",
    "db = next(db_gen)\n",
    "\n",
    "try:\n",
    "    # Count records before deletion\n",
    "    submissions_count = db.query(Submission).count()\n",
    "    assets_count = db.query(Asset).count()\n",
    "    interviews_count = db.query(ExitInterview).count()\n",
    "    reminders_count = db.query(ExitInterviewReminder).count()\n",
    "\n",
    "    print(f\"üìä Current Records Count:\")\n",
    "    print(f\"  Submissions: {submissions_count}\")\n",
    "    print(f\"  Assets: {assets_count}\")\n",
    "    print(f\"  Exit Interviews: {interviews_count}\")\n",
    "    print(f\"  Interview Reminders: {reminders_count}\")\n",
    "\n",
    "    if submissions_count == 0:\n",
    "        print(\"‚úÖ Submissions table is already empty!\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  WARNING: About to delete ALL submission records!\")\n",
    "\n",
    "        # Delete all records in correct order (respecting foreign keys)\n",
    "\n",
    "        # 1. Delete interview reminders first\n",
    "        db.query(ExitInterviewReminder).delete()\n",
    "        print(\"‚úÖ Deleted interview reminders\")\n",
    "\n",
    "        # 2. Delete exit interviews\n",
    "        db.query(ExitInterview).delete()\n",
    "        print(\"‚úÖ Deleted exit interviews\")\n",
    "\n",
    "        # 3. Delete assets\n",
    "        db.query(Asset).delete()\n",
    "        print(\"‚úÖ Deleted assets\")\n",
    "\n",
    "        # 4. Delete submissions last\n",
    "        db.query(Submission).delete()\n",
    "        print(\"‚úÖ Deleted submissions\")\n",
    "\n",
    "        # Commit the changes\n",
    "        db.commit()\n",
    "\n",
    "        print(\"\\nüéâ SUCCESS! All submission records have been deleted.\")\n",
    "\n",
    "        # Verify deletion\n",
    "        remaining_submissions = db.query(Submission).count()\n",
    "        print(f\"üìä Remaining submissions: {remaining_submissions}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {str(e)}\")\n",
    "    db.rollback()\n",
    "finally:\n",
    "    db.close()\n",
    "    print(\"üîê Database connection closed\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"‚úÖ Operation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc87c130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to PostgreSQL database...\n",
      "Starting asset table migration...\n",
      "--------------------------------------------------\n",
      "Adding new columns...\n",
      "‚úì New columns added\n",
      "\n",
      "Migrating existing data...\n",
      "‚úì Data migrated\n",
      "\n",
      "Dropping old columns...\n",
      "‚úì Old columns removed\n",
      "--------------------------------------------------\n",
      "‚úì Migration completed successfully!\n",
      "\n",
      "New assets table structure:\n",
      "  - id (integer) NOT NULL\n",
      "  - res_id (integer) NOT NULL\n",
      "  - created_at (timestamp with time zone) NOT NULL\n",
      "  - updated_at (timestamp with time zone) NOT NULL\n",
      "  - assets_returned (boolean) NULL\n",
      "  - notes (text) NULL\n",
      "\n",
      "Total asset records: 0\n",
      "\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "# Cell: Run Asset Table Migration (PostgreSQL)\n",
    "from sqlalchemy import create_engine, text\n",
    "from config import DATABASE_URL\n",
    "\n",
    "try:\n",
    "    print(\"Connecting to PostgreSQL database...\")\n",
    "    engine = create_engine(DATABASE_URL)\n",
    "\n",
    "    print(\"Starting asset table migration...\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # use a transaction so changes are committed/rolled back automatically\n",
    "    with engine.begin() as conn:\n",
    "        # Step 1: Add new columns\n",
    "        print(\"Adding new columns...\")\n",
    "        conn.execute(text(\"ALTER TABLE assets ADD COLUMN IF NOT EXISTS assets_returned BOOLEAN DEFAULT FALSE\"))\n",
    "        conn.execute(text(\"ALTER TABLE assets ADD COLUMN IF NOT EXISTS notes TEXT\"))\n",
    "        print(\"‚úì New columns added\")\n",
    "\n",
    "        # Step 2: Migrate existing data\n",
    "        print(\"\\nMigrating existing data...\")\n",
    "        conn.execute(text(\"UPDATE assets SET assets_returned = approved WHERE approved IS NOT NULL\"))\n",
    "\n",
    "        conn.execute(text(\"\"\"\n",
    "            UPDATE assets SET notes =\n",
    "                CASE\n",
    "                    WHEN laptop = TRUE OR mouse = TRUE OR headphones = TRUE OR others IS NOT NULL THEN\n",
    "                        'Items: ' ||\n",
    "                        CASE WHEN laptop = TRUE THEN 'Laptop, ' ELSE '' END ||\n",
    "                        CASE WHEN mouse = TRUE THEN 'Mouse, ' ELSE '' END ||\n",
    "                        CASE WHEN headphones = TRUE THEN 'Headphones, ' ELSE '' END ||\n",
    "                        COALESCE(others, '')\n",
    "                    ELSE NULL\n",
    "                END\n",
    "            WHERE laptop IS NOT NULL OR mouse IS NOT NULL OR headphones IS NOT NULL OR others IS NOT NULL\n",
    "        \"\"\"))\n",
    "        print(\"‚úì Data migrated\")\n",
    "\n",
    "        # Step 3: Drop old columns\n",
    "        print(\"\\nDropping old columns...\")\n",
    "        conn.execute(text(\"ALTER TABLE assets DROP COLUMN IF EXISTS laptop\"))\n",
    "        conn.execute(text(\"ALTER TABLE assets DROP COLUMN IF EXISTS mouse\"))\n",
    "        conn.execute(text(\"ALTER TABLE assets DROP COLUMN IF EXISTS headphones\"))\n",
    "        conn.execute(text(\"ALTER TABLE assets DROP COLUMN IF EXISTS others\"))\n",
    "        conn.execute(text(\"ALTER TABLE assets DROP COLUMN IF EXISTS approved\"))\n",
    "        print(\"‚úì Old columns removed\")\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "        print(\"‚úì Migration completed successfully!\")\n",
    "\n",
    "        # Show new table structure\n",
    "        result = conn.execute(text(\"\"\"\n",
    "            SELECT column_name, data_type, is_nullable\n",
    "            FROM information_schema.columns\n",
    "            WHERE table_name = 'assets'\n",
    "            ORDER BY ordinal_position\n",
    "        \"\"\"))\n",
    "        columns = result.fetchall()\n",
    "\n",
    "        print(\"\\nNew assets table structure:\")\n",
    "        for col in columns:\n",
    "            nullable = \"NULL\" if col[2] == \"YES\" else \"NOT NULL\"\n",
    "            print(f\"  - {col[0]} ({col[1]}) {nullable}\")\n",
    "\n",
    "        # Show record count\n",
    "        result = conn.execute(text(\"SELECT COUNT(*) FROM assets\"))\n",
    "        count = result.fetchone()[0]\n",
    "        print(f\"\\nTotal asset records: {count}\")\n",
    "\n",
    "    engine.dispose()\n",
    "    print(\"\\nDatabase connection closed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Migration failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "205cb152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal Server Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100    21  100    21    0     0     99      0 --:--:-- --:--:-- --:--:--   100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal Server Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100    21  100    21    0     0     99      0 --:--:-- --:--:-- --:--:--   100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal Server Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100    21  100    21    0     0     93      0 --:--:-- --:--:-- --:--:--    93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal Server Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100    21  100    21    0     0     98      0 --:--:-- --:--:-- --:--:--    99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal Server Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100    21  100    21    0     0     90      0 --:--:-- --:--:-- --:--:--    90\n"
     ]
    }
   ],
   "source": [
    "  # Email health check\n",
    "!curl http://localhost:8000/api/email-monitoring/email-health\n",
    "\n",
    "  # Delivery report (last 24 hours)\n",
    "!curl http://localhost:8000/api/email-monitoring/delivery-report?hours=24\n",
    "\n",
    "  # Failed emails\n",
    "!curl http://localhost:8000/api/email-monitoring/failed-emails\n",
    "\n",
    "  # Suspicious failures (silent failures detection)\n",
    "!curl http://localhost:8000/api/email-monitoring/suspicious-failures\n",
    "\n",
    "  # Quick stats\n",
    "!curl http://localhost:8000/api/email-monitoring/email-stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb66084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HR_Resignation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
